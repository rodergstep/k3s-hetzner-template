apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-kube-prometheus-stack-alertmanager
  namespace: prometheus-stack
  labels:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: kube-prometheus-stack
type: Opaque
stringData:
  alertmanager.yml: "global:\n  # Global SMTP configuration for email alerts\n  smtp_smarthost: 'localhost:587'\n  smtp_from: 'alertmanager@vanillax.me'\n  \n# Template configuration\ntemplates:\n  - '/etc/alertmanager/config/*.tmpl'\n\n# Routing configuration\nroute:\n  # Group alerts by cluster, alertname, and severity\n  group_by: ['cluster', 'alertname', 'severity']\n  \n  # Wait time before sending group notification\n  group_wait: 10s\n  \n  # Wait time before sending updates to the same group\n  group_interval: 5m\n  \n  # Wait time before sending notifications about new alerts\n  repeat_interval: 12h\n  \n  # Default receiver for unmatched alerts\n  receiver: 'web.hook'\n  \n  # Routing rules\n  routes:\n    # Critical alerts - immediate notification\n    - match:\n        severity: critical\n      receiver: 'critical-alerts'\n      group_wait: 5s\n      group_interval: 2m\n      repeat_interval: 5m\n      \n    # Warning alerts - less urgent\n    - match:\n        severity: warning\n      receiver: 'warning-alerts'\n      group_interval: 10m\n      repeat_interval: 2h\n      \n    # Infrastructure alerts\n    - match_re:\n        alertname: '^(NodeDown|KubeletDown|PrometheusDown)$'\n      receiver: 'infrastructure-alerts'\n      group_wait: 5s\n      repeat_interval: 10m\n\n# Inhibition rules - suppress certain alerts when others are firing\ninhibit_rules:\n  # Suppress warning alerts if critical alerts are firing\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['cluster', 'service']\n    \n  # Suppress node alerts if entire node is down\n  - source_match:\n      alertname: 'NodeDown'\n    target_match_re:\n      alertname: '^(NodeDiskRunningFull|NodeFilesystemSpaceFillingUp|NodeMemoryHighUtilization)$'\n    equal: ['instance']\n\n# Receivers configuration\nreceivers:\n  # Default webhook receiver (for unmatched alerts)\n  - name: 'web.hook'\n    webhook_configs:\n      - url: 'http://127.0.0.1:5001/'\n        send_resolved: true\n  \n  # Critical alerts receiver\n  - name: 'critical-alerts'\n    # Add your notification methods here:\n    # email_configs:\n    #   - to: 'admin@vanillax.me'\n    #     subject: 'CRITICAL: {{ .GroupLabels.alertname }} in {{ .GroupLabels.cluster }}'\n    #     body: |\n    #       {{ range .Alerts }}\n    #       Alert: {{ .Annotations.summary }}\n    #       Description: {{ .Annotations.description }}\n    #       Labels: {{ range .Labels.SortedPairs }}{{ .Name }}: {{ .Value }} {{ end }}\n    #       {{ end }}\n    # slack_configs:\n    #   - api_url: 'YOUR_SLACK_WEBHOOK_URL'\n    #     channel: '#alerts'\n    #     title: 'CRITICAL Alert'\n    #     text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'\n    webhook_configs:\n      - url: 'http://127.0.0.1:5001/'\n        send_resolved: true\n  \n  # Warning alerts receiver\n  - name: 'warning-alerts'\n    webhook_configs:\n      - url: 'http://127.0.0.1:5001/'\n        send_resolved: true\n  \n  # Infrastructure alerts receiver\n  - name: 'infrastructure-alerts'\n    webhook_configs:\n      - url: 'http://127.0.0.1:5001/'\n        send_resolved: true\n"
